{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "import argparse\r\n",
    "from collections import defaultdict\r\n",
    "\r\n",
    "import numpy as np\r\n",
    "\r\n",
    "import torch\r\n",
    "from transformers import BertConfig, BertTokenizerFast, TrainingArguments, Trainer\r\n",
    "\r\n",
    "from utils import seed_everything, empty_cuda_cache, compute_metrics\r\n",
    "from modeling import JointBERT\r\n",
    "from data_loader import LoadDataset\r\n",
    "from data_tokenizer import TokenizeDataset"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "parser = argparse.ArgumentParser()\r\n",
    "parser.add_argument('--task', default='snips')\r\n",
    "parser.add_argument('--epoch', default=30)\r\n",
    "parser.add_argument('--lr', default=5e-5)\r\n",
    "parser.add_argument('--batch', default=128)\r\n",
    "parser.add_argument('--seed', default=1234)\r\n",
    "args = parser.parse_args(args=[])\r\n",
    "\r\n",
    "TASK = args.task\r\n",
    "EPOCH = args.epoch\r\n",
    "LR = args.lr\r\n",
    "BATCH_SIZE = args.batch\r\n",
    "SEED = args.seed\r\n",
    "print('============================================================')\r\n",
    "print(f'TASK: {TASK}')\r\n",
    "print(f'EPOCH: {EPOCH}')\r\n",
    "print(f'LR: {LR}')\r\n",
    "print(f'BATCH_SIZE: {BATCH_SIZE}')\r\n",
    "print(f'SEED: {SEED}')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "============================================================\n",
      "TASK: snips\n",
      "EPOCH: 30\n",
      "LR: 5e-05\n",
      "BATCH_SIZE: 128\n",
      "SEED: 1234\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "seed_everything(SEED)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "seq_train = LoadDataset.load_dataset(f'./data/{TASK}/train/seq.in')\r\n",
    "seq_dev = LoadDataset.load_dataset(f'./data/{TASK}/dev/seq.in')\r\n",
    "seq_test = LoadDataset.load_dataset(f'./data/{TASK}/test/seq.in')\r\n",
    "\r\n",
    "intent_train = LoadDataset.load_dataset(f'./data/{TASK}/train/label')\r\n",
    "intent_dev = LoadDataset.load_dataset(f'./data/{TASK}/dev/label')\r\n",
    "intent_test = LoadDataset.load_dataset(f'./data/{TASK}/test/label')\r\n",
    "intent_labels = LoadDataset.load_dataset(f'./data/{TASK}/intent_label_vocab')\r\n",
    "\r\n",
    "slot_train = LoadDataset.load_dataset(f'./data/{TASK}/train/seq.out', slot = True)\r\n",
    "slot_dev = LoadDataset.load_dataset(f'./data/{TASK}/dev/seq.out', slot = True)\r\n",
    "slot_test = LoadDataset.load_dataset(f'./data/{TASK}/test/seq.out', slot = True)\r\n",
    "slot_labels = LoadDataset.load_dataset(f'./data/{TASK}/slot_label_vocab')\r\n",
    "\r\n",
    "intent_word2idx = defaultdict(int, {k: v for v, k in enumerate(intent_labels)})\r\n",
    "intent_idx2word = {v: k for v, k in enumerate(intent_labels)}\r\n",
    "\r\n",
    "slot_word2idx = defaultdict(int, {k: v for v, k in enumerate(slot_labels)})\r\n",
    "slot_idx2word = {v: k for v, k in enumerate(slot_labels)}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\r\n",
    "\r\n",
    "model_config = BertConfig.from_pretrained(\"bert-base-uncased\", num_labels = len(intent_idx2word), problem_type = \"single_label_classification\", id2label = intent_idx2word, label2id = intent_word2idx)\r\n",
    "# model_config.classifier_dropout\r\n",
    "\r\n",
    "model = JointBERT.from_pretrained(\"bert-base-uncased\", config = model_config, intent_labels = intent_labels, slot_labels = slot_labels)\r\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\r\n",
    "model.to(device);"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing JointBERT: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing JointBERT from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing JointBERT from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of JointBERT were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['slot_classifier.linear.weight', 'slot_classifier.linear.bias', 'intent_classifier.linear.bias', 'intent_classifier.linear.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "train_dataset = TokenizeDataset(seq_train, intent_train, slot_train, intent_word2idx, slot_word2idx, tokenizer)\r\n",
    "dev_dataset = TokenizeDataset(seq_dev, intent_dev, slot_dev, intent_word2idx, slot_word2idx, tokenizer)\r\n",
    "test_dataset = TokenizeDataset(seq_test, intent_test, slot_test, intent_word2idx, slot_word2idx, tokenizer)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "arguments = TrainingArguments(\r\n",
    "    output_dir='checkpoints',\r\n",
    "    do_train=True,\r\n",
    "    do_eval=True,\r\n",
    "\r\n",
    "    num_train_epochs=EPOCH,\r\n",
    "    learning_rate = LR,\r\n",
    "\r\n",
    "    save_strategy=\"epoch\",\r\n",
    "    save_total_limit=2,\r\n",
    "    evaluation_strategy=\"epoch\",\r\n",
    "    load_best_model_at_end=True,\r\n",
    "    \r\n",
    "    report_to = 'none',\r\n",
    "\r\n",
    "    per_device_train_batch_size=BATCH_SIZE,\r\n",
    "    per_device_eval_batch_size=64,\r\n",
    "    gradient_accumulation_steps=1,\r\n",
    "    dataloader_num_workers=0,\r\n",
    "    fp16=True,\r\n",
    "\r\n",
    ")\r\n",
    "\r\n",
    "trainer = Trainer(\r\n",
    "    model,\r\n",
    "    arguments,\r\n",
    "    train_dataset=train_dataset,\r\n",
    "    eval_dataset=dev_dataset\r\n",
    ")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Using cuda_amp half precision backend\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "empty_cuda_cache()\r\n",
    "trainer.train()\r\n",
    "model.save_pretrained(f\"checkpoints/{TASK}_ep{EPOCH}\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "c:\\Users\\HSC\\Documents\\VS_workspace\\pytorch17_cuda11\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 13084\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 128\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 128\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 103\n",
      " 99%|█████████▉| 102/103 [00:21<00:00,  4.91it/s]***** Running Evaluation *****\n",
      "  Num examples = 700\n",
      "  Batch size = 64\n",
      "\n",
      "100%|██████████| 103/103 [00:22<00:00,  4.91it/s]Saving model checkpoint to checkpoints\\checkpoint-103\n",
      "Configuration saved in checkpoints\\checkpoint-103\\config.json\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'eval_loss': 0.8551413416862488, 'eval_runtime': 0.323, 'eval_samples_per_second': 2167.149, 'eval_steps_per_second': 34.055, 'epoch': 1.0}\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Model weights saved in checkpoints\\checkpoint-103\\pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from checkpoints\\checkpoint-103 (score: 0.8551413416862488).\n",
      "100%|██████████| 103/103 [00:24<00:00,  4.19it/s]\n",
      "Configuration saved in checkpoints/snips_ep1\\config.json\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'train_runtime': 24.6002, 'train_samples_per_second': 531.865, 'train_steps_per_second': 4.187, 'train_loss': 2.2695216206670965, 'epoch': 1.0}\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Model weights saved in checkpoints/snips_ep1\\pytorch_model.bin\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "intent_label_ids = []\r\n",
    "slot_label_ids = []\r\n",
    "\r\n",
    "with open(f'./data/{TASK}/test/label', 'r', encoding='utf-8') as intent_f, \\\r\n",
    "    open(f'./data/{TASK}/test/seq.out', 'r', encoding='utf-8') as slot_f:\r\n",
    "    for line in intent_f:\r\n",
    "        line = line.strip()\r\n",
    "        intent_label_ids.append(line)\r\n",
    "    intent_label_ids = np.array(intent_label_ids)\r\n",
    "    \r\n",
    "    for line in slot_f:\r\n",
    "        line = line.strip().split()\r\n",
    "        slot_label_ids.append(line)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "def predict(model, seqs):\r\n",
    "    model.to('cpu')\r\n",
    "    pred_intent_ids = []\r\n",
    "    pred_slot_ids = []\r\n",
    "\r\n",
    "    for i in range(len(seqs)):\r\n",
    "        input_seq = tokenizer(seqs[i], return_tensors='pt')\r\n",
    "        \r\n",
    "        model.eval()\r\n",
    "        with torch.no_grad():\r\n",
    "            _, (intent_logits, slot_logits) = model(**input_seq)\r\n",
    "\r\n",
    "        # Intent\r\n",
    "        pred_intent_ids.append(intent_idx2word[intent_logits[0].argmax().item()])\r\n",
    "\r\n",
    "        # Slot\r\n",
    "        slot_logits_size = slot_logits[0].shape[0]\r\n",
    "        slot_logits_mask = np.array(test_dataset[i]['slot_label_ids'][:slot_logits_size]) != -100\r\n",
    "        slot_logits_clean = slot_logits[0][slot_logits_mask]\r\n",
    "        pred_slot_ids.append([slot_idx2word[i.item()] for i in slot_logits_clean.argmax(dim=1)])\r\n",
    "\r\n",
    "    return np.array(pred_intent_ids), pred_slot_ids\r\n",
    "\r\n",
    "pred_intent_ids, pred_slot_ids = predict(model, seq_test)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "res = compute_metrics(pred_intent_ids, intent_label_ids, pred_slot_ids, slot_label_ids)\r\n",
    "for k, v in res.items():\r\n",
    "    print(f'============{k}: {v}')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "============intent_acc: 0.9714285714285714\n",
      "============slot_precision: 0.6272727272727273\n",
      "============slot_recall: 0.693854748603352\n",
      "============slot_f1: 0.6588859416445624\n",
      "============sementic_frame_acc: 0.36\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('pytorch17_cuda11': venv)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  },
  "interpreter": {
   "hash": "6b370c5f79b32d0ca85832fe0eea34a8e089f2e478fb433874d6e8a8e5763002"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}