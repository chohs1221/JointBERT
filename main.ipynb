{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from transformers import BertConfig, BertTokenizerFast, TrainingArguments, Trainer\n",
    "\n",
    "from utils import seed_everything, empty_cuda_cache, compute_metrics\n",
    "from modeling import JointBERT\n",
    "from data_loader import LoadDataset\n",
    "from data_tokenizer import TokenizeDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TASK: snips\n",
      "EPOCH: 30\n",
      "LR: 5e-05\n",
      "BATCH_SIZE: 128\n",
      "SEED: 1234\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--task', default='snips')\n",
    "parser.add_argument('--epoch', default=30)\n",
    "parser.add_argument('--lr', default=5e-5)\n",
    "parser.add_argument('--batch', default=128)\n",
    "parser.add_argument('--seed', default=1234)\n",
    "args = parser.parse_args(args=[])\n",
    "\n",
    "TASK = args.task\n",
    "EPOCH = args.epoch\n",
    "LR = args.lr\n",
    "BATCH_SIZE = args.batch\n",
    "SEED = args.seed\n",
    "print('============================================================')\n",
    "print(f'TASK: {TASK}')\n",
    "print(f'EPOCH: {EPOCH}')\n",
    "print(f'LR: {LR}')\n",
    "print(f'BATCH_SIZE: {BATCH_SIZE}')\n",
    "print(f'SEED: {SEED}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_train = LoadDataset.load_dataset(f'./data/{TASK}/train/seq.in')\n",
    "seq_dev = LoadDataset.load_dataset(f'./data/{TASK}/dev/seq.in')\n",
    "seq_test = LoadDataset.load_dataset(f'./data/{TASK}/test/seq.in')\n",
    "\n",
    "intent_train = LoadDataset.load_dataset(f'./data/{TASK}/train/label')\n",
    "intent_dev = LoadDataset.load_dataset(f'./data/{TASK}/dev/label')\n",
    "intent_test = LoadDataset.load_dataset(f'./data/{TASK}/test/label')\n",
    "intent_labels = LoadDataset.load_dataset(f'./data/{TASK}/intent_label_vocab')\n",
    "\n",
    "slot_train = LoadDataset.load_dataset(f'./data/{TASK}/train/seq.out', slot = True)\n",
    "slot_dev = LoadDataset.load_dataset(f'./data/{TASK}/dev/seq.out', slot = True)\n",
    "slot_test = LoadDataset.load_dataset(f'./data/{TASK}/test/seq.out', slot = True)\n",
    "slot_labels = LoadDataset.load_dataset(f'./data/{TASK}/slot_label_vocab')\n",
    "\n",
    "intent_word2idx = defaultdict(int, {k: v for v, k in enumerate(intent_labels)})\n",
    "intent_idx2word = {v: k for v, k in enumerate(intent_labels)}\n",
    "\n",
    "slot_word2idx = defaultdict(int, {k: v for v, k in enumerate(slot_labels)})\n",
    "slot_idx2word = {v: k for v, k in enumerate(slot_labels)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing JointBERT: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing JointBERT from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing JointBERT from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of JointBERT were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['slot_classifier.linear.bias', 'intent_classifier.linear.bias', 'slot_classifier.linear.weight', 'intent_classifier.linear.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "model_config = BertConfig.from_pretrained(\"bert-base-uncased\", num_labels = len(intent_idx2word), problem_type = \"single_label_classification\", id2label = intent_idx2word, label2id = intent_word2idx)\n",
    "# model_config.classifier_dropout\n",
    "\n",
    "model = JointBERT.from_pretrained(\"bert-base-uncased\", config = model_config, intent_labels = intent_labels, slot_labels = slot_labels)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TokenizeDataset(seq_train, intent_train, slot_train, intent_word2idx, slot_word2idx, tokenizer)\n",
    "dev_dataset = TokenizeDataset(seq_dev, intent_dev, slot_dev, intent_word2idx, slot_word2idx, tokenizer)\n",
    "test_dataset = TokenizeDataset(seq_test, intent_test, slot_test, intent_word2idx, slot_word2idx, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cuda_amp half precision backend\n"
     ]
    }
   ],
   "source": [
    "arguments = TrainingArguments(\n",
    "    output_dir='checkpoints',\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "\n",
    "    num_train_epochs=EPOCH,\n",
    "    learning_rate = LR,\n",
    "\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    \n",
    "    report_to = 'none',\n",
    "\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=64,\n",
    "    gradient_accumulation_steps=1,\n",
    "    dataloader_num_workers=0,\n",
    "    fp16=True,\n",
    "\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    arguments,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=dev_dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HSC\\Documents\\VS_workspace\\pytorch17_cuda11\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 13084\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 128\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 128\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 103\n",
      " 99%|█████████▉| 102/103 [00:21<00:00,  4.91it/s]***** Running Evaluation *****\n",
      "  Num examples = 700\n",
      "  Batch size = 64\n",
      "\n",
      "100%|██████████| 103/103 [00:22<00:00,  4.91it/s]Saving model checkpoint to checkpoints\\checkpoint-103\n",
      "Configuration saved in checkpoints\\checkpoint-103\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8551413416862488, 'eval_runtime': 0.323, 'eval_samples_per_second': 2167.149, 'eval_steps_per_second': 34.055, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in checkpoints\\checkpoint-103\\pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from checkpoints\\checkpoint-103 (score: 0.8551413416862488).\n",
      "100%|██████████| 103/103 [00:24<00:00,  4.19it/s]\n",
      "Configuration saved in checkpoints/snips_ep1\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 24.6002, 'train_samples_per_second': 531.865, 'train_steps_per_second': 4.187, 'train_loss': 2.2695216206670965, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in checkpoints/snips_ep1\\pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "empty_cuda_cache()\n",
    "trainer.train()\n",
    "model.save_pretrained(f\"checkpoints/{TASK}_ep{EPOCH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# last_model = JointBERT.from_pretrained(\"./checkpoints/checkpoint-1050\", config = model_config, intent_labels = intent_labels, slot_labels = slot_labels)\n",
    "intent_label_ids = []\n",
    "slot_label_ids = []\n",
    "\n",
    "with open(f'./data/{TASK}/test/label', 'r', encoding='utf-8') as intent_f, \\\n",
    "    open(f'./data/{TASK}/test/seq.out', 'r', encoding='utf-8') as slot_f:\n",
    "    for line in intent_f:\n",
    "        line = line.strip()\n",
    "        intent_label_ids.append(line)\n",
    "    intent_label_ids = np.array(intent_label_ids)\n",
    "    \n",
    "    for line in slot_f:\n",
    "        line = line.strip().split()\n",
    "        slot_label_ids.append(line)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, seqs):\n",
    "    model.to('cpu')\n",
    "    pred_intent_ids = []\n",
    "    pred_slot_ids = []\n",
    "\n",
    "    for i in range(len(seqs)):\n",
    "        input_seq = tokenizer(seqs[i], return_tensors='pt')\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            _, (intent_logits, slot_logits) = model(**input_seq)\n",
    "\n",
    "        # Intent\n",
    "        pred_intent_ids.append(intent_idx2word[intent_logits[0].argmax().item()])\n",
    "\n",
    "        # Slot\n",
    "        slot_logits_size = slot_logits[0].shape[0]\n",
    "        slot_logits_mask = np.array(test_dataset[i]['slot_label_ids'][:slot_logits_size]) != -100\n",
    "        slot_logits_clean = slot_logits[0][slot_logits_mask]\n",
    "        pred_slot_ids.append([slot_idx2word[i.item()] for i in slot_logits_clean.argmax(dim=1)])\n",
    "\n",
    "    return np.array(pred_intent_ids), pred_slot_ids\n",
    "\n",
    "pred_intent_ids, pred_slot_ids = predict(model, seq_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============intent_acc: 0.9714285714285714\n",
      "============slot_precision: 0.6272727272727273\n",
      "============slot_recall: 0.693854748603352\n",
      "============slot_f1: 0.6588859416445624\n",
      "============sementic_frame_acc: 0.36\n"
     ]
    }
   ],
   "source": [
    "res = compute_metrics(pred_intent_ids, intent_label_ids, pred_slot_ids, slot_label_ids)\n",
    "for k, v in res.items():\n",
    "    print(f'{k:<20}: {v}')\n",
    "print(f'============================================================\\n\\n\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
