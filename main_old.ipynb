{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import os\r\n",
    "import random\r\n",
    "from collections import defaultdict\r\n",
    "\r\n",
    "import numpy as np\r\n",
    "\r\n",
    "import gc\r\n",
    "\r\n",
    "import torch\r\n",
    "import torch.nn as nn\r\n",
    "from transformers import BertConfig, BertModel, BertTokenizerFast, BertPreTrainedModel, TrainingArguments, Trainer\r\n",
    "\r\n",
    "from seqeval.metrics import precision_score, recall_score, f1_score"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "TASK = 'atis'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "def seed_everything(seed:int = 1004):\r\n",
    "    random.seed(seed)\r\n",
    "    np.random.seed(seed)\r\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\r\n",
    "    torch.manual_seed(seed)\r\n",
    "    torch.cuda.manual_seed(seed)  # type: ignore\r\n",
    "    torch.backends.cudnn.deterministic = True  # type: ignore\r\n",
    "    torch.backends.cudnn.benchmark = True  # type: ignore\r\n",
    "\r\n",
    "seed_everything(1234)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "class IntentClassifier(nn.Module):\r\n",
    "    def __init__(self, hidden_size, num_intent_labels, classifier_dropout):\r\n",
    "        super(IntentClassifier, self).__init__()\r\n",
    "        self.dropout = nn.Dropout(classifier_dropout)\r\n",
    "        self.linear = nn.Linear(hidden_size, num_intent_labels)\r\n",
    "\r\n",
    "    def forward(self, x):\r\n",
    "        x = self.dropout(x)\r\n",
    "        return self.linear(x)\r\n",
    "\r\n",
    "\r\n",
    "class SlotClassifier(nn.Module):\r\n",
    "    def __init__(self, hidden_size, num_slot_labels, classifier_dropout):\r\n",
    "        super(SlotClassifier, self).__init__()\r\n",
    "        self.dropout = nn.Dropout(classifier_dropout)\r\n",
    "        self.linear = nn.Linear(hidden_size, num_slot_labels)\r\n",
    "\r\n",
    "    def forward(self, x):\r\n",
    "        x = self.dropout(x)\r\n",
    "        return self.linear(x)\r\n",
    "    \r\n",
    "    \r\n",
    "class JointBERT(BertPreTrainedModel):\r\n",
    "    def __init__(self, config, intent_labels, slot_labels):\r\n",
    "        super().__init__(config)\r\n",
    "        self.num_intent_labels = len(intent_labels)\r\n",
    "        self.num_slot_labels = len(slot_labels)\r\n",
    "        self.config = config\r\n",
    "\r\n",
    "        self.bert = BertModel(config)\r\n",
    "\r\n",
    "        classifier_dropout = config.classifier_dropout if config.classifier_dropout is not None else config.hidden_dropout_prob\r\n",
    "        \r\n",
    "        self.intent_classifier = IntentClassifier(config.hidden_size, self.num_intent_labels, classifier_dropout)\r\n",
    "        self.slot_classifier = SlotClassifier(config.hidden_size, self.num_slot_labels, classifier_dropout)\r\n",
    "\r\n",
    "        # Initialize weights and apply final processing\r\n",
    "        self.post_init()\r\n",
    "\r\n",
    "    def forward(\r\n",
    "        self,\r\n",
    "        input_ids = None,\r\n",
    "        attention_mask = None,\r\n",
    "        token_type_ids = None,\r\n",
    "        position_ids = None,\r\n",
    "        head_mask = None,\r\n",
    "        inputs_embeds = None,\r\n",
    "        intent_label_ids = None,\r\n",
    "        slot_label_ids = None,\r\n",
    "        output_attentions = None,\r\n",
    "        output_hidden_states = None,\r\n",
    "        # return_dict = None\r\n",
    "        ):\r\n",
    "        # return_dict = return_dict if return_dict is not None else self.config.use_return_dict\r\n",
    "\r\n",
    "        outputs = self.bert(\r\n",
    "            input_ids,\r\n",
    "            attention_mask=attention_mask,\r\n",
    "            token_type_ids=token_type_ids,\r\n",
    "            position_ids=position_ids,\r\n",
    "            head_mask=head_mask,\r\n",
    "            inputs_embeds=inputs_embeds,\r\n",
    "            output_attentions=output_attentions,\r\n",
    "            output_hidden_states=output_hidden_states,\r\n",
    "            # return_dict=return_dict,\r\n",
    "        )   # sequence_output, pooled_output, (hidden_states), (attentions)\r\n",
    "\r\n",
    "        sequence_output = outputs[0]\r\n",
    "        pooled_output = outputs[1]  # [CLS]\r\n",
    "\r\n",
    "        intent_logits = self.intent_classifier(pooled_output)\r\n",
    "        slot_logits = self.slot_classifier(sequence_output)\r\n",
    "\r\n",
    "        total_loss = 0\r\n",
    "        # 1. Intent Softmax\r\n",
    "        if intent_label_ids is not None:\r\n",
    "            if self.num_intent_labels == 1:\r\n",
    "                intent_loss_fct = nn.MSELoss()\r\n",
    "                intent_loss = intent_loss_fct(intent_logits.squeeze(), intent_label_ids.squeeze())\r\n",
    "            else:\r\n",
    "                intent_loss_fct = nn.CrossEntropyLoss()\r\n",
    "                intent_loss = intent_loss_fct(intent_logits.view(-1, self.num_intent_labels), intent_label_ids.view(-1))\r\n",
    "            total_loss += intent_loss\r\n",
    "\r\n",
    "        # 2. Slot Softmax\r\n",
    "        if slot_label_ids is not None:\r\n",
    "            loss_fct = nn.CrossEntropyLoss()\r\n",
    "            slot_loss = loss_fct(slot_logits.view(-1, self.num_slot_labels), slot_label_ids.view(-1))\r\n",
    "            total_loss += slot_loss\r\n",
    "\r\n",
    "        outputs = ((intent_logits, slot_logits),) + outputs[2:]  # add hidden states and attention if they are here\r\n",
    "\r\n",
    "        outputs = (total_loss,) + outputs\r\n",
    "\r\n",
    "        return outputs  # (loss), logits, (hidden_states), (attentions) # Logits is a tuple of intent and slot logits\r\n",
    "    "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "class LoadDataset:\r\n",
    "    def __init__(self, data):\r\n",
    "        self.data = data\r\n",
    "        \r\n",
    "    @classmethod\r\n",
    "    def load_dataset(cls, file_name, slot = False):\r\n",
    "        data = []\r\n",
    "        with open(file_name, 'r', encoding='utf-8') as f:\r\n",
    "            for line in f:\r\n",
    "                line = line.strip()\r\n",
    "                if slot:\r\n",
    "                    line = line.split()\r\n",
    "                data.append(line)\r\n",
    "        \r\n",
    "        return cls(data)\r\n",
    "            \r\n",
    "\r\n",
    "    def __getitem__(self, index):\r\n",
    "        return self.data[index]\r\n",
    "    \r\n",
    "    def __len__(self):\r\n",
    "        return len(self.data)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "seq_train = LoadDataset.load_dataset(f'./data/{TASK}/train/seq.in')\r\n",
    "seq_dev = LoadDataset.load_dataset(f'./data/{TASK}/dev/seq.in')\r\n",
    "seq_test = LoadDataset.load_dataset(f'./data/{TASK}/test/seq.in')\r\n",
    "\r\n",
    "intent_train = LoadDataset.load_dataset(f'./data/{TASK}/train/label')\r\n",
    "intent_dev = LoadDataset.load_dataset(f'./data/{TASK}/dev/label')\r\n",
    "intent_test = LoadDataset.load_dataset(f'./data/{TASK}/test/label')\r\n",
    "intent_labels = LoadDataset.load_dataset(f'./data/{TASK}/intent_label_vocab')\r\n",
    "\r\n",
    "slot_train = LoadDataset.load_dataset(f'./data/{TASK}/train/seq.out', slot = True)\r\n",
    "slot_dev = LoadDataset.load_dataset(f'./data/{TASK}/dev/seq.out', slot = True)\r\n",
    "slot_test = LoadDataset.load_dataset(f'./data/{TASK}/test/seq.out', slot = True)\r\n",
    "slot_labels = LoadDataset.load_dataset(f'./data/{TASK}/slot_label_vocab')\r\n",
    "\r\n",
    "intent_word2idx = defaultdict(int, {k: v for v, k in enumerate(intent_labels)})\r\n",
    "intent_idx2word = {v: k for v, k in enumerate(intent_labels)}\r\n",
    "\r\n",
    "slot_word2idx = defaultdict(int, {k: v for v, k in enumerate(slot_labels)})\r\n",
    "slot_idx2word = {v: k for v, k in enumerate(slot_labels)}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "model_config = BertConfig.from_pretrained(\"bert-base-uncased\", num_labels = len(intent_idx2word), problem_type = \"single_label_classification\", id2label = intent_idx2word, label2id = intent_word2idx)\r\n",
    "# model_config.classifier_dropout"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "model = JointBERT.from_pretrained(\"bert-base-uncased\", config = model_config, intent_labels = intent_labels, slot_labels = slot_labels)\r\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\r\n",
    "model.to(device);"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing JointBERT: ['cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing JointBERT from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing JointBERT from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of JointBERT were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['intent_classifier.linear.weight', 'slot_classifier.linear.weight', 'slot_classifier.linear.bias', 'intent_classifier.linear.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "class TokenizeDataset:\r\n",
    "    def __init__(self, seqs, intent_labels, slot_labels, intent_word2idx, slot_word2idx, tokenizer):\r\n",
    "        self.seqs = seqs\r\n",
    "        self.intent_labels = intent_labels\r\n",
    "        self.slot_labels = slot_labels\r\n",
    "        \r\n",
    "        self.intent_word2idx = intent_word2idx\r\n",
    "        self.slot_word2idx = slot_word2idx\r\n",
    "        \r\n",
    "        self.tokenizer = tokenizer\r\n",
    "    \r\n",
    "    def align_label(self, seq, intent_label, slot_label):\r\n",
    "        tokens = self.tokenizer(seq, padding='max_length', max_length=50, truncation=True)\r\n",
    "        token_idxs = tokens.word_ids()\r\n",
    "        \r\n",
    "        pre_word_idx = None\r\n",
    "        slot_label_ids = []\r\n",
    "        for word_idx in token_idxs:\r\n",
    "            if word_idx != pre_word_idx:\r\n",
    "                try:\r\n",
    "                    slot_label_ids.append(slot_word2idx[slot_label[word_idx]])\r\n",
    "                except:\r\n",
    "                    slot_label_ids.append(-100)\r\n",
    "\r\n",
    "            elif word_idx == pre_word_idx or word_idx is None:\r\n",
    "                slot_label_ids.append(-100)\r\n",
    "\r\n",
    "            pre_word_idx = word_idx\r\n",
    "        \r\n",
    "        tokens['intent_label_ids'] = [intent_word2idx[intent_label]]\r\n",
    "        tokens['slot_label_ids'] = slot_label_ids\r\n",
    "        \r\n",
    "        return tokens\r\n",
    "\r\n",
    "    def __getitem__(self, index):\r\n",
    "        bert_input = self.align_label(self.seqs[index], self.intent_labels[index], self.slot_labels[index])\r\n",
    "        return bert_input\r\n",
    "    \r\n",
    "    def __len__(self):\r\n",
    "        return len(self.seqs)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "train_dataset = TokenizeDataset(seq_train, intent_train, slot_train, intent_word2idx, slot_word2idx, tokenizer)\r\n",
    "dev_dataset = TokenizeDataset(seq_dev, intent_dev, slot_dev, intent_word2idx, slot_word2idx, tokenizer)\r\n",
    "test_dataset = TokenizeDataset(seq_test, intent_test, slot_test, intent_word2idx, slot_word2idx, tokenizer)\r\n",
    "print(train_dataset[0])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'input_ids': [101, 1045, 2215, 2000, 4875, 2013, 6222, 2000, 5759, 2461, 4440, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'intent_label_ids': [13], 'slot_label_ids': [-100, 2, 2, 2, 2, 2, 73, 2, 115, 99, 100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]}\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "arguments = TrainingArguments(\r\n",
    "    output_dir='checkpoints',\r\n",
    "    do_train=True,\r\n",
    "    do_eval=True,\r\n",
    "\r\n",
    "    num_train_epochs=30,\r\n",
    "    learning_rate = 5e-5,\r\n",
    "\r\n",
    "    save_strategy=\"epoch\",\r\n",
    "    save_total_limit=2,\r\n",
    "    evaluation_strategy=\"epoch\",\r\n",
    "    load_best_model_at_end=True,\r\n",
    "    \r\n",
    "    report_to = 'none',\r\n",
    "\r\n",
    "    per_device_train_batch_size=128,\r\n",
    "    per_device_eval_batch_size=32,\r\n",
    "    gradient_accumulation_steps=1,\r\n",
    "    dataloader_num_workers=0,\r\n",
    "    fp16=True,\r\n",
    "\r\n",
    ")\r\n",
    "\r\n",
    "trainer = Trainer(\r\n",
    "    model,\r\n",
    "    arguments,\r\n",
    "    train_dataset=train_dataset,\r\n",
    "    eval_dataset=dev_dataset\r\n",
    ")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Using cuda_amp half precision backend\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "gc.collect()\r\n",
    "torch.cuda.empty_cache()\r\n",
    "trainer.train()\r\n",
    "model.save_pretrained(f\"checkpoints/first_checkpoint\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 4478\n",
      "  Num Epochs = 30\n",
      "  Instantaneous batch size per device = 128\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 128\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1050\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1050' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1050/1050 03:48, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.694382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.851170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.534896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.420869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.353123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.309167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.296346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.290570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.259210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.279477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.276411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.266315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.262811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.259829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.482200</td>\n",
       "      <td>0.270969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.482200</td>\n",
       "      <td>0.258914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.482200</td>\n",
       "      <td>0.266580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.482200</td>\n",
       "      <td>0.267345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.482200</td>\n",
       "      <td>0.268218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.482200</td>\n",
       "      <td>0.269249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.482200</td>\n",
       "      <td>0.270710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.482200</td>\n",
       "      <td>0.272677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.482200</td>\n",
       "      <td>0.276027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.482200</td>\n",
       "      <td>0.277243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.482200</td>\n",
       "      <td>0.271478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.482200</td>\n",
       "      <td>0.276759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.482200</td>\n",
       "      <td>0.275001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.482200</td>\n",
       "      <td>0.272594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.022200</td>\n",
       "      <td>0.277901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.022200</td>\n",
       "      <td>0.276848</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints/checkpoint-35\n",
      "Configuration saved in checkpoints/checkpoint-35/config.json\n",
      "Model weights saved in checkpoints/checkpoint-35/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints/checkpoint-515] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints/checkpoint-70\n",
      "Configuration saved in checkpoints/checkpoint-70/config.json\n",
      "Model weights saved in checkpoints/checkpoint-70/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints/checkpoint-3090] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints/checkpoint-105\n",
      "Configuration saved in checkpoints/checkpoint-105/config.json\n",
      "Model weights saved in checkpoints/checkpoint-105/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints/checkpoint-35] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints/checkpoint-140\n",
      "Configuration saved in checkpoints/checkpoint-140/config.json\n",
      "Model weights saved in checkpoints/checkpoint-140/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints/checkpoint-70] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints/checkpoint-175\n",
      "Configuration saved in checkpoints/checkpoint-175/config.json\n",
      "Model weights saved in checkpoints/checkpoint-175/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints/checkpoint-105] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints/checkpoint-210\n",
      "Configuration saved in checkpoints/checkpoint-210/config.json\n",
      "Model weights saved in checkpoints/checkpoint-210/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints/checkpoint-140] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints/checkpoint-245\n",
      "Configuration saved in checkpoints/checkpoint-245/config.json\n",
      "Model weights saved in checkpoints/checkpoint-245/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints/checkpoint-175] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints/checkpoint-280\n",
      "Configuration saved in checkpoints/checkpoint-280/config.json\n",
      "Model weights saved in checkpoints/checkpoint-280/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints/checkpoint-210] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints/checkpoint-315\n",
      "Configuration saved in checkpoints/checkpoint-315/config.json\n",
      "Model weights saved in checkpoints/checkpoint-315/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints/checkpoint-245] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints/checkpoint-350\n",
      "Configuration saved in checkpoints/checkpoint-350/config.json\n",
      "Model weights saved in checkpoints/checkpoint-350/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints/checkpoint-280] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints/checkpoint-385\n",
      "Configuration saved in checkpoints/checkpoint-385/config.json\n",
      "Model weights saved in checkpoints/checkpoint-385/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints/checkpoint-350] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints/checkpoint-420\n",
      "Configuration saved in checkpoints/checkpoint-420/config.json\n",
      "Model weights saved in checkpoints/checkpoint-420/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints/checkpoint-385] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints/checkpoint-455\n",
      "Configuration saved in checkpoints/checkpoint-455/config.json\n",
      "Model weights saved in checkpoints/checkpoint-455/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints/checkpoint-420] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints/checkpoint-490\n",
      "Configuration saved in checkpoints/checkpoint-490/config.json\n",
      "Model weights saved in checkpoints/checkpoint-490/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints/checkpoint-455] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints/checkpoint-525\n",
      "Configuration saved in checkpoints/checkpoint-525/config.json\n",
      "Model weights saved in checkpoints/checkpoint-525/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints/checkpoint-490] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints/checkpoint-560\n",
      "Configuration saved in checkpoints/checkpoint-560/config.json\n",
      "Model weights saved in checkpoints/checkpoint-560/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints/checkpoint-315] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints/checkpoint-595\n",
      "Configuration saved in checkpoints/checkpoint-595/config.json\n",
      "Model weights saved in checkpoints/checkpoint-595/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints/checkpoint-525] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints/checkpoint-630\n",
      "Configuration saved in checkpoints/checkpoint-630/config.json\n",
      "Model weights saved in checkpoints/checkpoint-630/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints/checkpoint-595] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints/checkpoint-665\n",
      "Configuration saved in checkpoints/checkpoint-665/config.json\n",
      "Model weights saved in checkpoints/checkpoint-665/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints/checkpoint-630] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints/checkpoint-700\n",
      "Configuration saved in checkpoints/checkpoint-700/config.json\n",
      "Model weights saved in checkpoints/checkpoint-700/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints/checkpoint-665] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints/checkpoint-735\n",
      "Configuration saved in checkpoints/checkpoint-735/config.json\n",
      "Model weights saved in checkpoints/checkpoint-735/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints/checkpoint-700] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints/checkpoint-770\n",
      "Configuration saved in checkpoints/checkpoint-770/config.json\n",
      "Model weights saved in checkpoints/checkpoint-770/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints/checkpoint-735] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints/checkpoint-805\n",
      "Configuration saved in checkpoints/checkpoint-805/config.json\n",
      "Model weights saved in checkpoints/checkpoint-805/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints/checkpoint-770] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints/checkpoint-840\n",
      "Configuration saved in checkpoints/checkpoint-840/config.json\n",
      "Model weights saved in checkpoints/checkpoint-840/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints/checkpoint-805] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints/checkpoint-875\n",
      "Configuration saved in checkpoints/checkpoint-875/config.json\n",
      "Model weights saved in checkpoints/checkpoint-875/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints/checkpoint-840] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints/checkpoint-910\n",
      "Configuration saved in checkpoints/checkpoint-910/config.json\n",
      "Model weights saved in checkpoints/checkpoint-910/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints/checkpoint-875] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints/checkpoint-945\n",
      "Configuration saved in checkpoints/checkpoint-945/config.json\n",
      "Model weights saved in checkpoints/checkpoint-945/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints/checkpoint-910] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints/checkpoint-980\n",
      "Configuration saved in checkpoints/checkpoint-980/config.json\n",
      "Model weights saved in checkpoints/checkpoint-980/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints/checkpoint-945] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints/checkpoint-1015\n",
      "Configuration saved in checkpoints/checkpoint-1015/config.json\n",
      "Model weights saved in checkpoints/checkpoint-1015/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints/checkpoint-980] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to checkpoints/checkpoint-1050\n",
      "Configuration saved in checkpoints/checkpoint-1050/config.json\n",
      "Model weights saved in checkpoints/checkpoint-1050/pytorch_model.bin\n",
      "Deleting older checkpoint [checkpoints/checkpoint-1015] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from checkpoints/checkpoint-560 (score: 0.25891417264938354).\n",
      "Configuration saved in checkpoints/first_checkpoint/config.json\n",
      "Model weights saved in checkpoints/first_checkpoint/pytorch_model.bin\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "source": [
    "intent_label_ids = []\r\n",
    "slot_label_ids = []\r\n",
    "\r\n",
    "with open(f'./data/{TASK}/test/label', 'r', encoding='utf-8') as intent_f, \\\r\n",
    "    open(f'./data/{TASK}/test/seq.out', 'r', encoding='utf-8') as slot_f:\r\n",
    "    for line in intent_f:\r\n",
    "        line = line.strip()\r\n",
    "        intent_label_ids.append(line)\r\n",
    "    for line in slot_f:\r\n",
    "        line = line.strip().split()\r\n",
    "        slot_label_ids.append(line)\r\n",
    "        # slot_label_ids.append(np.array(line))\r\n",
    "\r\n",
    "intent_label_ids = np.array(intent_label_ids)\r\n",
    "# slot_label_ids = np.array(slot_label_ids)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "source": [
    "def predict(model, seqs):\r\n",
    "    model.to('cpu')\r\n",
    "    pred_intent_ids = []\r\n",
    "    pred_slot_ids = []\r\n",
    "\r\n",
    "    for i in range(len(seqs)):\r\n",
    "        input_seq = tokenizer(seq_test[i], return_tensors='pt')\r\n",
    "        \r\n",
    "        model.eval()\r\n",
    "        with torch.no_grad():\r\n",
    "            _, (intent_logits, slot_logits) = model(**input_seq)\r\n",
    "\r\n",
    "        # Intent\r\n",
    "        pred_intent_ids.append(intent_idx2word[intent_logits[0].argmax().item()])\r\n",
    "\r\n",
    "        # Slot\r\n",
    "        slot_logits_size = slot_logits[0].shape[0]\r\n",
    "        slot_logits_mask = np.array(test_dataset[i]['slot_label_ids'][:slot_logits_size]) != -100\r\n",
    "        slot_logits_clean = slot_logits[0][slot_logits_mask]\r\n",
    "        pred_slot_ids.append([slot_idx2word[i.item()] for i in slot_logits_clean.argmax(dim=1)])\r\n",
    "\r\n",
    "    return np.array(pred_intent_ids), pred_slot_ids"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "source": [
    "# last_model = JointBERT.from_pretrained(\"./checkpoints/checkpoint-1050\", config = model_config, intent_labels = intent_labels, slot_labels = slot_labels)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "source": [
    "pred_intent_ids, pred_slot_ids = predict(model, seq_test)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "source": [
    "def get_intent_acc(preds, labels):\r\n",
    "    acc = (preds == labels).mean()\r\n",
    "    return {\r\n",
    "        \"intent_acc\": acc\r\n",
    "    }\r\n",
    "\r\n",
    "def get_slot_metrics(preds, labels):\r\n",
    "    assert len(preds) == len(labels)\r\n",
    "    return {\r\n",
    "        \"slot_precision\": precision_score(labels, preds),\r\n",
    "        \"slot_recall\": recall_score(labels, preds),\r\n",
    "        \"slot_f1\": f1_score(labels, preds)\r\n",
    "    }\r\n",
    "\r\n",
    "def get_sentence_frame_acc(intent_preds, intent_labels, slot_preds, slot_labels):\r\n",
    "    \"\"\"For the cases that intent and all the slots are correct (in one sentence)\"\"\"\r\n",
    "    # Get the intent comparison result\r\n",
    "    intent_result = (intent_preds == intent_labels)\r\n",
    "\r\n",
    "    # Get the slot comparision result\r\n",
    "    slot_result = []\r\n",
    "    for preds, labels in zip(slot_preds, slot_labels):\r\n",
    "        assert len(preds) == len(labels)\r\n",
    "        one_sent_result = True\r\n",
    "        for p, l in zip(preds, labels):\r\n",
    "            if p != l:\r\n",
    "                one_sent_result = False\r\n",
    "                break\r\n",
    "        slot_result.append(one_sent_result)\r\n",
    "    slot_result = np.array(slot_result)\r\n",
    "\r\n",
    "    sementic_acc = np.multiply(intent_result, slot_result).mean()\r\n",
    "    return {\r\n",
    "        \"sementic_frame_acc\": sementic_acc\r\n",
    "    }\r\n",
    "\r\n",
    "def compute_metrics(intent_preds, intent_labels, slot_preds, slot_labels):\r\n",
    "    assert len(intent_preds) == len(intent_labels) == len(slot_preds) == len(slot_labels)\r\n",
    "    \r\n",
    "    results = {}\r\n",
    "    intent_result = get_intent_acc(intent_preds, intent_labels)\r\n",
    "    print(intent_result)\r\n",
    "    slot_result = get_slot_metrics(slot_preds, slot_labels)\r\n",
    "    print(slot_result)\r\n",
    "    sementic_result = get_sentence_frame_acc(intent_preds, intent_labels, slot_preds, slot_labels)\r\n",
    "    print(sementic_result)\r\n",
    "\r\n",
    "    results.update(intent_result)\r\n",
    "    results.update(slot_result)\r\n",
    "    results.update(sementic_result)\r\n",
    "\r\n",
    "    return results"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "source": [
    "res = compute_metrics(pred_intent_ids, intent_label_ids, pred_slot_ids, slot_label_ids)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'intent_acc': 0.007838745800671893}\n",
      "{'slot_precision': 0.0016810489745601256, 'slot_recall': 0.005287275290800141, 'slot_f1': 0.0025510204081632655}\n",
      "{'sementic_frame_acc': 0.0}\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('pytorch17_cuda11': venv)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  },
  "interpreter": {
   "hash": "6b370c5f79b32d0ca85832fe0eea34a8e089f2e478fb433874d6e8a8e5763002"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}